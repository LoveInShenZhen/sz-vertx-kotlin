# 负载均衡+高可用

## 负载均衡方案

* 针对 **无状态的服务** 进行负载均衡
* 负载均衡采用 **HAproxy** 方式实现
* 参考资料:
  * [HAproxy 官网](http://www.haproxy.org/)  _需要翻墙_
  * [HAproxy GitHub](https://github.com/haproxy/haproxy)
  * [HAProxy用法详解](https://www.bookstack.cn/read/HAProxy-zh/readme.md)
  * [Haproxy 快速指南](https://www.jianshu.com/p/baa296770bd5)

## 高可用方案

* 高可用采用 **keepalived** 方式实现
* 参考资料:
  * [Keepalive 学习资料](https://www.bookstack.cn/read/huweihuang-linux-notes/keepalived-keepalived-introduction.md)
  * KEEPALIVED 权威指南􏷧

## 部署方案示例

以下示例, 假定服务器部署在 **192.168.3.0/24** 的子网内

### IP地址规划

#### 行情分发服务

行情分发服务有 **3台** 服务器, 需要配置 **3 个固定IP** 和 **1 个虚IP**

| IP地址        | 说明                      |
| ------------- | ------------------------- |
| 192.168.3.161 | 行情分发服务集群 **虚IP** |
| 192.168.3.151 | 行情分发服务器 1 固定IP   |
| 192.168.3.152 | 行情分发服务器 2 固定IP   |
| 192.168.3.153 | 行情分发服务器 3 固定IP   |

#### 历史行情查询服务

历史行情查询服务有2台服务器, 需要配置 **2 个固定IP** 和 **1 个虚IP**

| IP地址        | 说明                          |
| ------------- | ----------------------------- |
| 192.168.3.162 | 历史行情查询服务集群 **虚IP** |
| 192.168.3.154 | 历史行情查询服务器 1 固定IP   |
| 192.168.3.155 | 历史行情查询服务器 2 固定IP   |

#### 接入网关

在东财的网络环境里, 是有一个 **F5防火墙** 作为网关的. 在测试环境里, 我们用一个 运行 Nginx 的虚拟机来模拟流量网关, 进行流量转发. 

| IP地址        | 说明                 |
| ------------- | -------------------- |
| 192.168.3.200 | 流量网关服务器固定IP |

### 搭建本地虚拟机测试环境

使用 **vagrant**  来搭建本地的测试环境. 本文是在 **Mac 系统 + VirtualBox** 环境下完成测试环境的搭建的, 使用 windows 平台的同学可以用来参考, 并根据实际情况进行调整

* 安装和配置请参考 [vagrant 官网](https://www.vagrantup.com/)

* 创建测试环境目录

  ```bash
  mkdir testenv
  ```

* 下载 ubuntu server 18.04 的 vagrant box

  > 官网很慢, 所以从中科大开源镜像站点下载

  ```bash
  vagrant box add https://mirrors.ustc.edu.cn/ubuntu-cloud-images/bionic/current/bionic-server-cloudimg-amd64-vagrant.box --name ubuntu/1804
  ```

* 初始化 vagrant 环境

  ```bash
  cd testenv
  vagrant init ubuntu/1804
  ```

  执行完命令后,  会在 testenv 目录下生成一个 **Vagrantfile** 文件. 
  
* 根据 **IP地址规划**, 实验环境需要创建 6 台虚拟机. 我们据此修改 **Vagrantfile** 文件如下:

  ```ruby
  # -*- mode: ruby -*-
  # vi: set ft=ruby :
  
  # All Vagrant configuration is done below. The "2" in Vagrant.configure
  # configures the configuration version (we support older styles for
  # backwards compatibility). Please don't change it unless you know what
  # you're doing.
  Vagrant.configure("2") do |config|
    # The most common configuration options are documented and commented below.
    # For a complete reference, please see the online documentation at
    # https://docs.vagrantup.com.
  
    # Every Vagrant development environment requires a box. You can search for
    # boxes at https://vagrantcloud.com/search.
    config.vm.box = "ubuntu/1804"
  
    # Disable automatic box update checking. If you disable this, then
    # boxes will only be checked for updates when the user runs
    # `vagrant box outdated`. This is not recommended.
    # config.vm.box_check_update = false
  
    # Create a forwarded port mapping which allows access to a specific port
    # within the machine from a port on the host machine. In the example below,
    # accessing "localhost:8080" will access port 80 on the guest machine.
    # NOTE: This will enable public access to the opened port
    # config.vm.network "forwarded_port", guest: 80, host: 8080
  
    # Create a forwarded port mapping which allows access to a specific port
    # within the machine from a port on the host machine and only allow access
    # via 127.0.0.1 to disable public access
    # config.vm.network "forwarded_port", guest: 80, host: 8080, host_ip: "127.0.0.1"
  
    # Create a private network, which allows host-only access to the machine
    # using a specific IP.
    # config.vm.network "private_network", ip: "192.168.33.10"
  
    # Create a public network, which generally matched to bridged network.
    # Bridged networks make the machine appear as another physical device on
    # your network.
    # config.vm.network "public_network"
  
    # Share an additional folder to the guest VM. The first argument is
    # the path on the host to the actual folder. The second argument is
    # the path on the guest to mount the folder. And the optional third
    # argument is a set of non-required options.
    # config.vm.synced_folder "../data", "/vagrant_data"
  
    # Provider-specific configuration so you can fine-tune various
    # backing providers for Vagrant. These expose provider-specific options.
    # Example for VirtualBox:
    #
    # config.vm.provider "virtualbox" do |vb|
    #   # Display the VirtualBox GUI when booting the machine
    #   vb.gui = true
    #
    #   # Customize the amount of memory on the VM:
    #   vb.memory = "1024"
    # end
    #
    # View the documentation for the provider you are using for more
    # information on available options.
  
    # Enable provisioning with a shell script. Additional provisioners such as
    # Ansible, Chef, Docker, Puppet and Salt are also available. Please see the
    # documentation for more information about their specific syntax and use.
    # config.vm.provision "shell", inline: <<-SHELL
    #   apt-get update
    #   apt-get install -y apache2
    # SHELL
  
    machine_list = [
      "192.168.3.151",
      "192.168.3.152",
      "192.168.3.153",
      "192.168.3.154",
      "192.168.3.155"
    ]
  
    machine_list.each do |vm_ip| 
      config.vm.define "vm_#{vm_ip}" do |vm_instance|
        vm_instance.vm.network "private_network", ip: "#{vm_ip}"
      end
    end
  
    config.vm.define "vm_192.168.3.200" do |vm_instance|
      vm_instance.vm.network "private_network", ip: "192.168.3.200"
      vm_instance.vm.network "public_network" 
    end
  
    config.vm.provider "virtualbox" do |vb|
      vb.memory = 512
      vb.cpus = 2
    end
  
  end
  
  ```

* 启动 6 台虚拟

  ```bash
  vagrant up
  ```

* 查看启动的虚拟机实例列表, 应该可以看到有 6 台虚拟机处于 running 状态.

  ```
  $ vagrant status
  Current machine states:
  
  vm_192.168.3.151          running (virtualbox)
  vm_192.168.3.152          running (virtualbox)
  vm_192.168.3.153          running (virtualbox)
  vm_192.168.3.154          running (virtualbox)
  vm_192.168.3.155          running (virtualbox)
  vm_192.168.3.200          running (virtualbox)
  
  This environment represents multiple VMs. The VMs are all listed
  above with their current state. For more information about a specific
  VM, run `vagrant status NAME`.
  ```

### 部署高可用 keepalived

#### 安装keepalived

* 分别 ssh 登录到 **3 台行情分发服务器** 和 **2 台历史行情查询服务器** 上安装 keepalived

* 下面以 **行情分发服务器 1** 为例:

  > 注: vagrant 下是使用如下命令登录到 vargrant 创建的虚拟机里, 并且是以 **vagrant** 这个用户登录进去的. 该用户的密码也是 **vagrant**

  ```bash
  vagrant ssh vm_192.168.3.151
  ```

* ssh 登录成功后, 执行如下命令安装 keepalived

  >  为加速下载apt包, 修改ubuntu 源为阿里源

  ```bash
  sudo rm /etc/apt/sources.list && sudo vim /etc/apt/sources.list
  ```

  修改为如下:

  ```
  deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
  
  deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
  
  deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
  
  deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
  
  deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
  
  ```

  更新 apt 并 安装 keepalived

  ```bash
  sudo apt-get update && sudo apt-get install -y keepalived
  ```

#### 配置 keepalived

* 配置文件路径: **/etc/keepalived/keepalived.conf**

##### 行情分发服务 keepalived 配置

* 行情分发服务的虚IP地址: **192.168.3.161/24**

* 3 台高可用节点, IP 地址分别为: 192.168.3.151, 192.168.3.152, 192.168.3.153

* 查看虚拟机上, 绑定 IP 地址的网卡名称, 可以看到虚拟机里网卡设备名称为: **enp0s8**, 下面 **keepalived.conf** 配置文件会用到

  ```
  $ ip a
  1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
      inet 127.0.0.1/8 scope host lo
         valid_lft forever preferred_lft forever
      inet6 ::1/128 scope host
         valid_lft forever preferred_lft forever
  2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
      link/ether 02:a9:c7:4f:3f:23 brd ff:ff:ff:ff:ff:ff
      inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
         valid_lft 81394sec preferred_lft 81394sec
      inet6 fe80::a9:c7ff:fe4f:3f23/64 scope link
         valid_lft forever preferred_lft forever
  3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
      link/ether 08:00:27:5a:62:ee brd ff:ff:ff:ff:ff:ff
      inet 192.168.3.151/24 brd 192.168.3.255 scope global enp0s8
         valid_lft forever preferred_lft forever
      inet6 fe80::a00:27ff:fe5a:62ee/64 scope link
         valid_lft forever preferred_lft forever
  
  ```

  

* 修改配置文件 **/etc/keepalived/keepalived.conf** 为如下内容: 

  ```bash
  sudo vim /etc/keepalived/keepalived.conf
  ```
  
  
  
  ```
  global_defs {
    default_interface enp0s8
  }
  
  vrrp_instance VI_1 {
    # interface 为虚IP所绑定的网卡设备名称
    interface enp0s8
  
    state BACKUP
    # VRID􏶖􏳑(0...255), 每个虚IP需要对应一个不同的数字, 行情分发服务用 99, 历史行情查询服务使用 66
    virtual_router_id 99
    priority 100
    advert 1
    nopreempt
  
    # 后端服务的 EndPoint 列表
    unicast_peer {
      192.168.3.151
      192.168.3.152
      192.168.3.153
    }
  
    # 虚IP地址, 地址上包括掩码
    virtual_ipaddress {
      192.168.3.161/24
    }
  
    authentication {
      auth_type PASS
      auth_pass myquant
    }
  
  }
  ```
  
* 重启 keepalived 服务

  ```bash
  sudo systemctl restart keepalived
  ```

* 分别在 3 台行情分发服务器上执行以上操作



##### 历史行情查询服务 keepalived 配置

* 历史行情查询服务的虚IP地址: **192.168.3.162/24**

* 2 台高可用节点: 192.168.3.154, 192.168.3.154

* 修改配置文件 **/etc/keepalived/keepalived.conf** 为如下内容: 
  ```bash
  sudo vim /etc/keepalived/keepalived.conf
  ```
  
  ```
  global_defs {
    default_interface enp0s8
  }
  
  vrrp_instance VI_1 {
    # interface 为虚IP所绑定的网卡设备名称
    interface enp0s8
  
    state BACKUP
    # VRID􏶖􏳑(0...255), 每个虚IP需要对应一个不同的数字, 行情分发服务用 99, 历史行情查询服务使用 66
    virtual_router_id 66
    priority 100
    advert 1
    nopreempt
  
    # 后端服务的 EndPoint 列表
    unicast_peer {
      192.168.3.154
      192.168.3.155
    }
  
    # 虚IP地址, 地址上包括掩码
    virtual_ipaddress {
      192.168.3.162/24
    }
  
    authentication {
      auth_type PASS
      auth_pass myquant
    }
  
  }
  ```

* 重启 keepalived 服务

  ```bash
  sudo systemctl restart keepalived
  ```

* 分别在 2 台历史行情查询服务器上执行以上操作

### 测试高可用

ToDo

### 部署 HAproxy

#### 安装 HAproxy

* 分别 ssh 登录到 **3 台行情分发服务器** 和 **2 台历史行情查询服务器** 上安装 HAproxy

  ```bash
  sudo apt-get install -y haproxy
  ```

#### 配置 HAProxy

* 配置文件路径: **/etc/haproxy/haproxy.cfg**

##### 行情分发服务 HAproxy 配置

* 3 台负载均衡节点, IP 地址分别为: 192.168.3.151, 192.168.3.152, 192.168.3.153
* 因为 HAproxy 与提供服务(响应请求负载) 是部署在同一台机器上的, 所以为了避免端口冲突, 负载均衡上 **暴露服务的端口号** 和 **服务程序的端口号** 是不一样的.
* 本示例中, 负载均衡上 **暴露服务的端口号** 为 **9000**, **服务程序的端口号** 为 **8080**

##### 行情分发服务 HAproxy 配置

* 修改配置文件 **/etc/haproxy/haproxy.cfg** 为如下内容: 

  ```bash
  sudo rm /etc/haproxy/haproxy.cfg && sudo vim /etc/haproxy/haproxy.cfg
  ```

  ```
  global
    log /dev/log	local0
    log /dev/log	local1 notice
  
  defaults
    log global
    option dontlognull
    # Set the maximum time to wait for a connection attempt to a server to succeed.
    timeout connect 3000ms
    # Set the maximum inactivity time on the client side.
    timeout client  60s
    # Set the maximum inactivity time on the server side.
    timeout server  120s
  
  # 开启 haproxy stats 页面, 方便调试和监控
  listen stats 
      # haproxy stats 页面的服务端口, 注意检查不要端口冲突被占用, 可以换成其他端口
      bind *:1936
      mode http
      stats enable
      stats hide-version
      stats realm Haproxy\ Statistics
      # haproxy stats 页面的 url
      stats uri /haproxy_stats
      # haproxy stats 页面的访问的用户名和密码
      stats auth startcraft:showmethemoney
  
  # 定义一个服务前端, 名称为 quotation_service
  frontend quotation_service
      # 网络协议为 tcp
      mode tcp
      # 负载均衡对外提供服务的端口
      bind *:9000
      # 该服务前端对应的后端配置
      default_backend quotation_backend
  
  backend quotation_backend
      mode tcp
      balance roundrobin
      stick-table type ip size 200k expire 30m
      # stick 为 on 则开启会话保持
      #stick on src
      server server1 192.168.3.151:8080 check inter 1s rise 3 fall 1
      server server2 192.168.3.152:8080 check inter 1s rise 3 fall 1
      server server3 192.168.3.153:8080 check inter 1s rise 3 fall 1
  ```
  
* 重启 haproxy 服务

  ```bash
  sudo systemctl restart haproxy
  ```

### 模拟后端API服务器

#### 模拟行情分发服务器

* ssh 登录到主机上

* 创建测试目录

  ```bash
  mkdir -p ~/test
  ```
  
* 创建一个静态文件

  ```bash
  vim ~/test/index.html
  ```

  文件内容如下:

  ```html
  <html>
    <header>
      <meta http-equiv="Content-Type" content="text/html;charset=utf-8">
    </header>
    <body>
      <p>行情分发服务器 1</p>
    </body>
  </html>
  ```

* 启动一个 http server, 端口号: 8080

  ```
  cd ~/test
  python3 -m http.server 8080
  ```

  终端不要关闭, http server 的进程在前台执行

* 在另外2台行情服务器主机上, 重复以上步骤, 注意要修改 **~/test/index.html** 文件内容中的服务器编号, 用来在测试中反应是由哪台服务器响应请求的, 验证负载均衡服务是否生效

### 模拟接入网关

* ssh 登录到 **192.168.3.200**

* 安装 nginx

  ```bash
  sudo apt-get update && sudo apt-get install -y nginx-full
  ```

* 查看该主机在办公网络里的IP地址

  ```bash
  vagrant@ubuntu-bionic:~$ ip a
  1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
      inet 127.0.0.1/8 scope host lo
         valid_lft forever preferred_lft forever
      inet6 ::1/128 scope host
         valid_lft forever preferred_lft forever
  2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
      link/ether 02:a9:c7:4f:3f:23 brd ff:ff:ff:ff:ff:ff
      inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
         valid_lft 85991sec preferred_lft 85991sec
      inet6 fe80::a9:c7ff:fe4f:3f23/64 scope link
         valid_lft forever preferred_lft forever
  3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
      link/ether 08:00:27:10:9b:d7 brd ff:ff:ff:ff:ff:ff
      inet 192.168.3.200/24 brd 192.168.3.255 scope global enp0s8
         valid_lft forever preferred_lft forever
      inet6 fe80::a00:27ff:fe10:9bd7/64 scope link
         valid_lft forever preferred_lft forever
  4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
      link/ether 08:00:27:4e:9d:25 brd ff:ff:ff:ff:ff:ff
      inet 192.168.0.148/24 brd 192.168.0.255 scope global dynamic enp0s9
         valid_lft 490sec preferred_lft 490sec
      inet6 fe80::a00:27ff:fe4e:9d25/64 scope link
         valid_lft forever preferred_lft forever
  ```

  可以看到, 网卡设备 **enp0s9** 分配了办公网络的IP地址: 192.168.0.148

* 查看 nginx 服务状态

  ```bash
  sudo systemctl status nginx
  ```

* 在浏览器查看: http://192.168.0.148

* 配置流量端口转发

  | 网关暴露端口 | 用途                             | 转发到 IP:Port     |
  | ------------ | -------------------------------- | ------------------ |
  | 8080         | 行情服务                         | 192.168.3.161:9000 |
  | 1936         | 行情服务负载均衡监控页面         | 192.168.3.161:1936 |
  | 8090         | 历史行情查询服务                 | 192.168.3.162:9100 |
  | 1937         | 历史行情查询服务负载均衡监控页面 | 192.168.3.162:1936 |
  |              |                                  |                    |
  |              |                                  |                    |
  |              |                                  |                    |

  ```bash
  sudo vim /etc/nginx/conf.d/myquant.conf
  ```

  ```nginx
  server {
      # 端口, 请根据实际情况进行修改, 并对应的修改 sz-docker-compose.yml
      listen       1936;
  
      proxy_http_version 1.1;
      proxy_set_header   Host             $http_host;
      proxy_set_header   X-Real-IP        $remote_addr;
      proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
  
      client_max_body_size 100m;
      client_body_buffer_size 100m;
      client_header_timeout 60s;
      client_body_timeout 600s;
      proxy_connect_timeout 60s;
      proxy_read_timeout 60s;
      proxy_send_timeout 600s;
  
      #if_modified_since before;
  
      #access_log  logs/host.access.log  main;
  
      location / {
          proxy_pass   http://192.168.3.161:1936;
      }
  
      #error_page  404              /404.html;
  
      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
          root   html;
      }
  }
  
  server {
      # 端口, 请根据实际情况进行修改, 并对应的修改 sz-docker-compose.yml
      listen       8080;
  
      proxy_http_version 1.1;
      proxy_set_header   Host             $http_host;
      proxy_set_header   X-Real-IP        $remote_addr;
      proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
  
      client_max_body_size 100m;
      client_body_buffer_size 100m;
      client_header_timeout 60s;
      client_body_timeout 600s;
      proxy_connect_timeout 60s;
      proxy_read_timeout 60s;
      proxy_send_timeout 600s;
  
      #if_modified_since before;
  
      #access_log  logs/host.access.log  main;
  
      location / {
          proxy_pass   http://192.168.3.161:9000;
      }
  
      #error_page  404              /404.html;
  
      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
          root   html;
      }
  }
  
  server {
      # 端口, 请根据实际情况进行修改, 并对应的修改 sz-docker-compose.yml
      listen       8090;
  
      proxy_http_version 1.1;
      proxy_set_header   Host             $http_host;
      proxy_set_header   X-Real-IP        $remote_addr;
      proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
  
      client_max_body_size 100m;
      client_body_buffer_size 100m;
      client_header_timeout 60s;
      client_body_timeout 600s;
      proxy_connect_timeout 60s;
      proxy_read_timeout 60s;
      proxy_send_timeout 600s;
  
      #if_modified_since before;
  
      #access_log  logs/host.access.log  main;
  
      location / {
          proxy_pass   http://192.168.3.162:9100;
      }
  
      #error_page  404              /404.html;
  
      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
          root   html;
      }
  }
  
  server {
      # 端口, 请根据实际情况进行修改, 并对应的修改 sz-docker-compose.yml
      listen       1937;
  
      proxy_http_version 1.1;
      proxy_set_header   Host             $http_host;
      proxy_set_header   X-Real-IP        $remote_addr;
      proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
  
      client_max_body_size 100m;
      client_body_buffer_size 100m;
      client_header_timeout 60s;
      client_body_timeout 600s;
      proxy_connect_timeout 60s;
      proxy_read_timeout 60s;
      proxy_send_timeout 600s;
  
      #if_modified_since before;
  
      #access_log  logs/host.access.log  main;
  
      location / {
          proxy_pass   http://192.168.3.162:1937;
      }
  
      #error_page  404              /404.html;
  
      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
          root   html;
      }
  }
  ```

  